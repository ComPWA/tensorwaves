{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%config Completer.use_jedi = False\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import os\n",
    "\n",
    "STATIC_WEB_PAGE = {\"EXECUTE_NB\", \"READTHEDOCS\"}.intersection(os.environ)\n",
    "\n",
    "# Install on Google Colab\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "install_packages = \"google.colab\" in str(get_ipython())\n",
    "if install_packages:\n",
    "    for package in [\"tensorwaves[doc]\", \"graphviz\"]:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorWaves with AmpForm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(compwa-step-1)=\n",
    "## Step 1: Create amplitude model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether {ref}`generating data <compwa-step-2>` or {ref}`fitting a model <compwa-step-3>`, TensorWaves takes mathematical expressions as input. When that expression is an amplitude model, it is most convenient to formulate it with {mod}`ampform`.\n",
    "\n",
    "This notebook illustrates how to create such an amplitude model with {mod}`ampform` and how to write it to a recipe file that can be understood by {mod}`tensorwaves`. For more advanced examples, have a look at {doc}`the usage guides of AmpForm <ampform:usage>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the helicity formalism, but you can also use `formalism=\"canonical-helicity\"`. As you can see, we analyze the decay $J/\\psi \\to \\pi^0\\pi^0\\gamma$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Simplified model: $J/\\psi \\to f_0\\gamma$\n",
    "---\n",
    "class: dropdown\n",
    "---\n",
    "As {ref}`compwa-step-3` serves to illustrate usage only, we make the amplitude model here a bit simpler by not allowing $\\omega$ resonances (which are narrow and therefore hard to fit). For this reason, we can also limit the {class}`~qrules.settings.InteractionType` to {attr}`~qrules.settings.InteractionType.STRONG`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qrules\n",
    "\n",
    "reaction = qrules.generate_transitions(\n",
    "    initial_state=(\"J/psi(1S)\", [-1, +1]),\n",
    "    final_state=[\"gamma\", \"pi0\", \"pi0\"],\n",
    "    allowed_intermediate_particles=[\"f(0)\"],\n",
    "    allowed_interaction_types=[\"strong\", \"EM\"],\n",
    "    formalism=\"helicity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a small goodie, you can use [`graphviz`](https://pypi.org/project/graphviz) to {doc}`visualize <qrules:usage/visualize>` the generated graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "dot = qrules.io.asdot(reaction, collapse_graphs=True)\n",
    "Source(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert the {attr}`~qrules.transition.ReactionInfo.transitions` into an amplitude model (here: {class}`~ampform.helicity.HelicityModel`). This can be done with {func}`~ampform.get_builder` and {meth}`~ampform.helicity.HelicityAmplitudeBuilder.formulate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampform\n",
    "\n",
    "model_builder = ampform.get_builder(reaction)\n",
    "model = model_builder.formulate()\n",
    "display(*model.parameter_defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart of the model is a sympy expression that contains the full description of the intensity model. Note two things:\n",
    "1. The coefficients for the different amplitudes are **complex** valued.\n",
    "2. By default there is no dynamics in the model, so it still has to be specified.\n",
    "\n",
    "We choose to use {func}`~ampform.dynamics.relativistic_breit_wigner_with_ff` as the lineshape for all resonances and use a Blatt-Weisskopf form factor ({func}`~ampform.dynamics.builder.create_non_dynamic_with_ff`) for the production decay. The {meth}`~ampform.helicity.HelicityAmplitudeBuilder.set_dynamics` is a convenience interface for replacing the dynamics for intermediate states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampform.dynamics.builder import (\n",
    "    create_non_dynamic_with_ff,\n",
    "    create_relativistic_breit_wigner_with_ff,\n",
    ")\n",
    "\n",
    "model_builder.set_dynamics(\"J/psi(1S)\", create_non_dynamic_with_ff)\n",
    "for name in reaction.get_intermediate_particles().names:\n",
    "    model_builder.set_dynamics(name, create_relativistic_breit_wigner_with_ff)\n",
    "model = model_builder.formulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take another look at the parameters of the model to see which new parameters are there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "sorted(model.parameter_defaults, key=lambda s: s.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the {class}`~ampform.helicity.HelicityModel` to disk via {mod}`pickle`. The {class}`~qrules.transition.ReactionInfo` object can be pickled as well, but here, we write it to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "qrules.io.write(reaction, \"transitions.json\")\n",
    "with open(\"helicity_model.pickle\", \"wb\") as stream:\n",
    "    pickle.dump(model, stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that's it! We now have a template for an amplitude model with which to {ref}`generate data <compwa-step-2>` and {ref}`perform a fit <compwa-step-3>`. In the next steps, we will use use this {class}`~ampform.helicity.HelicityModel` as a fit model template for {mod}`tensorwaves`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(compwa-step-2)=\n",
    "## Step 2: Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the {class}`~ampform.helicity.HelicityModel` that we created with {mod}`ampform` in {ref}`the previous step <compwa-step-1>` to generate a data sample via hit & miss Monte Carlo. We do this with the {mod}`.data` module.\n",
    "\n",
    "First, we {func}`~pickle.load` the {class}`~ampform.helicity.HelicityModel` that was created in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from ampform.helicity import HelicityModel\n",
    "\n",
    "with open(\"helicity_model.pickle\", \"rb\") as model_file:\n",
    "    model: HelicityModel = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "reaction_info = model.reaction_info\n",
    "initial_state = next(iter(reaction_info.initial_state.values()))\n",
    "print(\"Initial state:\")\n",
    "print(\" \", initial_state.name)\n",
    "print(\"Final state:\")\n",
    "for i, p in reaction_info.final_state.items():\n",
    "    print(f\"  {i}: {p.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Generate phase space sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {class}`~qrules.transition.ReactionInfo` class defines the constraints of the phase space. As such, we have enough information to generate a **phase-space sample** for this particle reaction. We do this with the {func}`.generate_phsp` function. By default, this function uses {class}`.TFPhaseSpaceGenerator` as a, well... phase-space generator (using {obj}`tensorflow <tf.Tensor>` and the [`phasespace`](https://phasespace.readthedocs.io) package as a back-end) and generates random numbers with {class}`.TFUniformRealNumberGenerator`. You can use other generators with the arguments of {func}`.generate_phsp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to the main {ref}`usage:Generate data sample` of the main usage example page, we will generate a **deterministic** data sample. This can be done by feeding a {class}`.UniformRealNumberGenerator` with a specific {attr}`~.UniformRealNumberGenerator.seed` and feeding that generator to the {func}`.generate_phsp` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorwaves.data import TFUniformRealNumberGenerator, generate_phsp\n",
    "\n",
    "rng = TFUniformRealNumberGenerator(seed=0)\n",
    "initial_state_mass = reaction_info.initial_state[-1].mass\n",
    "final_state_masses = {i: p.mass for i, p in reaction_info.final_state.items()}\n",
    "phsp_momenta = generate_phsp(\n",
    "    size=100_000,\n",
    "    initial_state_mass=initial_state_mass,\n",
    "    final_state_masses=final_state_masses,\n",
    "    random_generator=rng,\n",
    ")\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        (k, label): np.transpose(v)[i]\n",
    "        for k, v in phsp_momenta.items()\n",
    "        for i, label in enumerate([\"E\", \"px\", \"py\", \"pz\"])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The resulting phase space sample is a {obj}`dict` of final state IDs to an {obj}`~numpy.array` of four-momenta. In the last step, we converted this sample in such a way that it is rendered as an understandable {class}`pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Generate intensity-based sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Data samples' are more complicated than phase space samples in that they represent the intensity profile resulting from a reaction. You therefore need a {class}`.Function` object that expresses an intensity distribution as well as a phase space over which to generate that distribution. We call such a data sample an **intensity-based sample**.\n",
    "\n",
    "An intensity-based sample is generated with the function {func}`.generate_data`. Its usage is similar to {func}`.generate_phsp`, but now you have to provide a {obj}`.Function` as well as a {obj}`.DataTransformer` that is used to transform the four-momentum phase space sample to a data sample that can be understood by the {obj}`.Function`.\n",
    "\n",
    "Now, recall that in {ref}`compwa-step-1`, we used the helicity formalism to mathematically express the reaction in terms of an amplitude model. TensorWaves needs to convert this {obj}`~ampform.helicity.HelicityModel` to a {obj}`.Function` object that can perform fast computations. This can be done with {func}`.create_parametrized_function`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{margin}\n",
    "\n",
    "Here, we make use of {func}`.fast_lambdify` by specifying `max_complexity`. See {ref}`usage/faster-lambdify:Specifying complexity`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-flake8"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorwaves.function.sympy import create_parametrized_function\n",
    "\n",
    "intensity = create_parametrized_function(\n",
    "    expression=model.expression.doit(),\n",
    "    parameters=model.parameter_defaults,\n",
    "    max_complexity=200,\n",
    "    backend=\"numpy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem is that {class}`.ParametrizedBackendFunction` takes a {obj}`.DataSample` with kinematic variables for the helicity formalism as input, not a set of four-momenta. We therefore need to construct a {class}`.DataTransformer` to transform these four-momenta to function variables. In this case, we work with the helicity formalism, so we construct a {class}`.SympyDataTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorwaves.data.transform import SympyDataTransformer\n",
    "\n",
    "helicity_transformer = SympyDataTransformer.from_sympy(\n",
    "    model.kinematic_variables, backend=\"jax\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, now we have enough info to create an intensity-based data sample. Notice how the structure of the output data is the same as the {ref}`phase-space sample we generated previously <usage/ampform:2.1 Generate phase space sample>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorwaves.data import generate_data\n",
    "\n",
    "data_momenta = generate_data(\n",
    "    size=10_000,\n",
    "    initial_state_mass=initial_state_mass,\n",
    "    final_state_masses=final_state_masses,\n",
    "    data_transformer=helicity_transformer,\n",
    "    intensity=intensity,\n",
    "    random_generator=rng,\n",
    ")\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        (k, label): np.transpose(v)[i]\n",
    "        for k, v in data_momenta.items()\n",
    "        for i, label in enumerate([\"E\", \"px\", \"py\", \"pz\"])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we use a {class}`.UniformRealNumberGenerator` with a specific {attr}`~.UniformRealNumberGenerator.seed` to ensure we get a **deterministic** data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize kinematic variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a phase space sample and an intensity-based sample. Their data structure isn't the most informative though: it's just a collection of four-momentum tuples. But we can again use the {class}`.SympyDataTransformer` to convert these four-momenta to (in the case of the helicity formalism) invariant masses and helicity angles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phsp = helicity_transformer(phsp_momenta)\n",
    "data = helicity_transformer(data_momenta)\n",
    "list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {obj}`.DataSample` is a mapping of kinematic variables names to a 1-dimensional array of values. The numbers you see here are final state IDs as defined in the {class}`~ampform.helicity.HelicityModel` member of the {class}`~ampform.helicity.HelicityModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for state_id, particle in reaction_info.final_state.items():\n",
    "    print(f\"ID {state_id}:\", particle.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "````{admonition} Available kinematic variables\n",
    "---\n",
    "class: dropdown\n",
    "---\n",
    "By default, {mod}`tensorwaves` only generates invariant masses of the {class}`Topologies <qrules.topology.Topology>` that are of relevance to the decay problem. In this case, we only have resonances $f_0 \\to \\pi^0\\pi^0$. If you are interested in more invariant mass combinations, you can do so with the method {meth}`~ampform.kinematics.HelicityAdapter.register_topology`.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {obj}`.DataSample` can easily be converted to a {class}`pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_frame = pd.DataFrame(data)\n",
    "phsp_frame = pd.DataFrame(data)\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means that we can use all kinds of fancy plotting functionality of for instance {mod}`matplotlib.pyplot` to see what's going on. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "resonances = sorted(\n",
    "    reaction_info.get_intermediate_particles(),\n",
    "    key=lambda p: p.mass,\n",
    ")\n",
    "\n",
    "evenly_spaced_interval = np.linspace(0, 1, len(resonances))\n",
    "colors = [cm.rainbow(x) for x in evenly_spaced_interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_frame[\"m_12\"].hist(bins=100, alpha=0.5, density=True, figsize=(9, 4))\n",
    "plt.xlabel(\"$m$ [GeV]\")\n",
    "for p, color in zip(resonances, colors):\n",
    "    plt.axvline(x=p.mass, linestyle=\"dotted\", label=p.name, color=color)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    "\n",
    "{ref}`usage/ampform:Intensity components`\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Export data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the generated data samples, simply {func}`pickle.dump` them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data.pickle\", \"wb\") as stream:\n",
    "    pickle.dump(data, stream)\n",
    "with open(\"phsp.pickle\", \"wb\") as stream:\n",
    "    pickle.dump(phsp, stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the {ref}`next step <compwa-step-3>`, we illustrate how to {meth}`~.Minuit2.optimize` the intensity model to these data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(compwa-step-3)=\n",
    "## Step 3: Perform fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the {ref}`previous step <compwa-step-2>`, a {class}`.ParametrizedFunction` can compute a list of intensities (real numbers) for an input {obj}`.DataSample`. At this stage, we want to optimize the parameters of this {class}`.ParametrizedFunction`, so that it matches the distribution of our data sample. This is what we call 'fitting'.\n",
    "\n",
    "First, we load the relevant data from the previous steps. Notice that we use {func}`.create_parametrized_function` with the argument `max_complexity`, which speeds up lambdification (see {doc}`/usage/faster-lambdify`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import qrules\n",
    "from ampform.helicity import HelicityModel\n",
    "\n",
    "from tensorwaves.function.sympy import create_parametrized_function\n",
    "\n",
    "reaction = qrules.io.load(\"transitions.json\")\n",
    "with open(\"helicity_model.pickle\", \"rb\") as stream:\n",
    "    model: HelicityModel = pickle.load(stream)\n",
    "with open(\"data.pickle\", \"rb\") as stream:\n",
    "    data = pickle.load(stream)\n",
    "with open(\"phsp.pickle\", \"rb\") as stream:\n",
    "    phsp = pickle.load(stream)\n",
    "\n",
    "function = create_parametrized_function(\n",
    "    expression=model.expression.doit(),\n",
    "    parameters=model.parameter_defaults,\n",
    "    backend=\"jax\",\n",
    "    max_complexity=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a fit, you need to define an {class}`.Estimator`. This is a measure for the discrepancy between the {class}`.ParametrizedFunction` and the data distribution to which you fit it. In PWA, we usually use an **unbinned negative log likelihood estimator** ({class}`.UnbinnedNLL`).\n",
    "\n",
    "Generally, the {class}`.ParametrizedFunction` is not normalized with regards to the data sample, while a log likelihood estimator requires a normalized function. This is where the {ref}`phase space data <usage/ampform:2.1 Generate phase space sample>` comes into play again: the {class}`.ParametrizedFunction` is evaluated over the phase space data, so that its output can be used as a normalization factor.\n",
    "\n",
    "```{margin}\n",
    "If you want to correct for the efficiency of the detector, you should use a *detector-reconstructed* phase space sample.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorwaves.estimator import UnbinnedNLL\n",
    "\n",
    "estimator = UnbinnedNLL(\n",
    "    function,\n",
    "    data=data,\n",
    "    phsp=phsp,\n",
    "    backend=\"jax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the {class}`.UnbinnedNLL` can be expressed with different backends, because it uses statistical operations like `log` and `mean`. Here, we use {func}`jax <jax.jit>`, which turns out to be the fastest backend for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimize intensity model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify fit parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the fit itself is quite simple: just create an {mod}`.optimizer` instance of your choice, here [Minuit2](https://root.cern.ch/doc/master/Minuit2Page.html), and call its {meth}`~.Optimizer.optimize` method to start the fitting process. The {meth}`~.Optimizer.optimize` method requires a mapping of parameter names to their initial values. **Only the parameters listed in the mapping are optimized.**\n",
    "\n",
    "Let's first select a few of parameters and give them a small offset with regard to their original value. We give this offset to make the fit more interestingâ€•after all, we are fitting to a data sample that was generated with this very same {class}`.ParametrizedFunction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "original_parameters = function.parameters\n",
    "initial_parameters = {\n",
    "    R\"C_{J/\\psi(1S) \\to f_{0}(1500)_{0} \\gamma_{+1}; f_{0}(1500) \\to \\pi^{0}_{0} \\pi^{0}_{0}}\": 1.0\n",
    "    + 0.0j,\n",
    "    \"Gamma_f(0)(980)\": 0.15,\n",
    "    \"Gamma_f(0)(1500)\": 0.2,\n",
    "    \"m_f(0)(1710)\": 1.78,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a {class}`.ParametrizedFunction` object computes the intensity for a certain `.DataSample`. This can be seen now nicely when we use these intensities as weights on the phase space sample and plot it together with the original data sample. Here, we look at the invariant mass distribution projection of the final states `1` and `2`, which, {ref}`as we saw before <usage/ampform:2.3 Visualize kinematic variables>`, is the final state particle pair $\\pi^0\\pi^0$.\n",
    "\n",
    "Don't forget to use {meth}`~.ParametrizedFunction.update_parameters` first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "reaction_info = model.reaction_info\n",
    "resonances = sorted(\n",
    "    reaction_info.get_intermediate_particles(),\n",
    "    key=lambda p: p.mass,\n",
    ")\n",
    "\n",
    "evenly_spaced_interval = np.linspace(0, 1, len(resonances))\n",
    "colors = [cm.rainbow(x) for x in evenly_spaced_interval]\n",
    "\n",
    "\n",
    "def indicate_masses():\n",
    "    plt.xlabel(\"$m$ [GeV]\")\n",
    "    for color, resonance in zip(colors, resonances):\n",
    "        plt.gca().axvline(\n",
    "            x=resonance.mass,\n",
    "            linestyle=\"dotted\",\n",
    "            label=resonance.name,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "\n",
    "def compare_model(\n",
    "    variable_name,\n",
    "    data,\n",
    "    phsp,\n",
    "    function,\n",
    "    bins=100,\n",
    "):\n",
    "    intensities = function(phsp)\n",
    "    _, ax = plt.subplots(figsize=(9, 4))\n",
    "    data_1d = data[variable_name]\n",
    "    ax = plt.gca()\n",
    "    ax.hist(\n",
    "        data_1d,\n",
    "        bins=bins,\n",
    "        alpha=0.5,\n",
    "        label=\"data\",\n",
    "        density=True,\n",
    "    )\n",
    "    phsp_1d = phsp[variable_name]\n",
    "    ax.hist(\n",
    "        phsp_1d,\n",
    "        weights=np.array(intensities),\n",
    "        bins=bins,\n",
    "        histtype=\"step\",\n",
    "        color=\"red\",\n",
    "        label=\"initial fit model\",\n",
    "        density=True,\n",
    "    )\n",
    "    indicate_masses()\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "function.update_parameters(initial_parameters)\n",
    "compare_model(\"m_12\", data, phsp, function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Custom callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {class}`.Minuit2` class allows one to insert certain {mod}`~tensorwaves.optimizer.callbacks`. Callbacks are used to insert behavior into the {meth}`.Optimizer.optimize` method. The {mod}`~tensorwaves.optimizer.callbacks` module provides several handy callback classes, but it's also possible to define custom callbacks into the {class}`.Optimizer` by defining a custom {class}`.Callback` class. Here's one that live updates a plot of the latest fit model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorwaves.optimizer.callbacks import Callback\n",
    "\n",
    "\n",
    "class PyplotCallback(Callback):\n",
    "    def __init__(self, variable=\"m_12\", step_size=10):\n",
    "        self.__variable = variable\n",
    "        self.__step_size = step_size\n",
    "        self.__fig = None\n",
    "        self.__ax = None\n",
    "        self.__latest_parameters = {}\n",
    "\n",
    "    def on_optimize_start(self, logs):\n",
    "        if STATIC_WEB_PAGE:\n",
    "            return\n",
    "        self.__fig, self.__ax = plt.subplots(1, figsize=(8, 5))\n",
    "\n",
    "    def on_optimize_end(self, logs):\n",
    "        if STATIC_WEB_PAGE:\n",
    "            return\n",
    "        self.update_plot(nbins=200)\n",
    "        self.__ax = None\n",
    "        self.__fig = None\n",
    "\n",
    "    def on_iteration_end(self, iteration, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_function_call_end(self, function_call, logs):\n",
    "        if STATIC_WEB_PAGE:\n",
    "            return\n",
    "        self.__latest_parameters = logs[\"parameters\"]\n",
    "        if function_call % self.__step_size != 0:\n",
    "            return\n",
    "        if function_call > 75:\n",
    "            return\n",
    "        self.update_plot(nbins=80)\n",
    "        clear_output(wait=True)\n",
    "        display(plt.gcf())\n",
    "\n",
    "    def update_plot(self, nbins: int):\n",
    "        if self.__fig is None or self.__ax is None:\n",
    "            self.__fig, self.__ax = plt.subplots(1, figsize=(8, 5))\n",
    "        function.update_parameters(self.__latest_parameters)\n",
    "        intensities = function(phsp)\n",
    "        self.__ax.cla()\n",
    "        data_1d = data[self.__variable]\n",
    "        self.__ax.hist(\n",
    "            data_1d, bins=nbins, alpha=0.5, label=\"data\", density=True\n",
    "        )\n",
    "        phsp_1d = phsp[self.__variable]\n",
    "        self.__ax.hist(\n",
    "            phsp_1d,\n",
    "            weights=np.array(intensities),\n",
    "            bins=nbins,\n",
    "            histtype=\"step\",\n",
    "            color=\"red\",\n",
    "            label=\"fit model\",\n",
    "            density=True,\n",
    "        )\n",
    "        self.__ax.set_xlim((0.25, 2.5))\n",
    "        self.__ax.set_ylim((0, 1.9))\n",
    "        indicate_masses()\n",
    "        plt.gcf().legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fit example below, we want to use several callbacks together. To do so, we 'stack' them together with {class}`.CallbackList`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorwaves.optimizer.callbacks import (\n",
    "    CallbackList,\n",
    "    CSVSummary,\n",
    "    TFSummary,\n",
    "    YAMLSummary,\n",
    ")\n",
    "\n",
    "callbacks = CallbackList(\n",
    "    [\n",
    "        CSVSummary(\"fit_traceback_minuit.csv\"),\n",
    "        PyplotCallback(),  # comment this line for raw fit performance\n",
    "        TFSummary(),\n",
    "        YAMLSummary(\"current_fit_result.yaml\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Perform fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create an {class}`.Optimizer` to {meth}`~.Minuit2.optimize` the parameters in the {class}`.ParametrizedFunction` (which is embedded in the {class}`.Estimator`). Here, we choose the {class}`.Minuit2` optimizer, which is the most common optimizer in high-energy physics. Since we constructed the {class}`.UnbinnedNLL` with {obj}`jax <jax.jit>`, can an optimize the model with analytic gradient over the {class}`.ParametrizedBackendFunction`:\n",
    "\n",
    "```{margin}\n",
    "The computation time depends on the complexity of the model, the number of data events, the size of the phase space sample, and the number of free parameters. This model is rather small and has but a few free parameters, so the optimization shouldn't take more than a minute.\n",
    "\n",
    "The custom callback example slows down the optimization, but you can remove it to get the raw fit performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorwaves.optimizer import Minuit2\n",
    "\n",
    "minuit2 = Minuit2(\n",
    "    callback=callbacks,\n",
    "    use_analytic_gradient=False,\n",
    ")\n",
    "fit_result = minuit2.optimize(estimator, initial_parameters)\n",
    "fit_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert fit_result.minimum_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the values of the optimized parameters in the {class}`.FitResult` are again comparable to the original parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the {mod}`.optimizer`s offer more specific information about the fit result. This information can be accessed with {attr}`.FitResult.specifics`. A common example would be to get the {attr}`~iminuit.Minuit.covariance` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = fit_result.specifics.covariance\n",
    "covariance_matrix.correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AIC and BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, here is how to compute the [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion#Definition) and [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion#Definition) from this {obj}`.FitResult`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_real_par = fit_result.count_number_of_parameters(complex_twice=True)\n",
    "n_events = len(list(data.values())[0])\n",
    "log_likelihood = -fit_result.estimator_value\n",
    "\n",
    "bic = n_real_par * np.log(n_events) - 2 * log_likelihood\n",
    "aic = 2 * n_real_par - 2 * log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"AIC:\", aic)\n",
    "print(\"BIC:\", bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "optimized_parameters = fit_result.parameter_values\n",
    "for p in optimized_parameters:\n",
    "    print(p)\n",
    "    print(f\"  initial:   {initial_parameters[p]:.3}\")\n",
    "    print(f\"  optimized: {optimized_parameters[p]:.3}\")\n",
    "    print(f\"  original:  {original_parameters[p]:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    "\n",
    "{ref}`usage/ampform:SciPy optimizer`\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Export and import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In {ref}`usage/ampform:3.2 Optimize intensity model`, we initialized {obj}`.Minuit2` with some callbacks that are {class}`.Loadable`. Such callback classes offer the possibility to {meth}`~.Loadable.load_latest_parameters`, so you can pick up the optimize process in case it crashes or if you pause it. Loading the latest parameters goes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_parameters = YAMLSummary.load_latest_parameters(\n",
    "    \"current_fit_result.yaml\"\n",
    ")\n",
    "latest_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To restart the fit with the latest parameters, simply rerun as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minuit2 = Minuit2()\n",
    "fit_result = minuit2.optimize(estimator, latest_parameters)\n",
    "fit_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert fit_result.minimum_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo and behold: the parameters were already optimized, so the fit converged faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot optimized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same method as above, we renew the parameters of the {class}`.ParametrizedFunction` and plot it again over the phase space sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "function.update_parameters(latest_parameters)\n",
    "compare_model(\"m_12\", data, phsp, function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intensity components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the {class}`~ampform.helicity.HelicityModel` contains {attr}`~ampform.helicity.HelicityModel.components` attribute. This is {obj}`dict` of component names to {class}`sympy.Expr <sympy.core.expr.Expr>`s helps us to identify amplitudes and intensities in the total amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "sorted(model.components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "model.components[\n",
    "    R\"A_{J/\\psi(1S)_{+1} \\to f_{0}(1370)_{0} \\gamma_{+1}; f_{0}(1370)_{0} \\to\"\n",
    "    R\" \\pi^{0}_{0} \\pi^{0}_{0}}\"\n",
    "].subs(model.parameter_defaults).doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in {ref}`usage/ampform:2.2 Generate intensity-based sample`, these _intensity components_ can each be expressed in a computational backend. This can be done with the method {meth}`~ampform.helicity.HelicityModel.sum_components` from the original model and creating a new {class}`.ParametrizedBackendFunction` from that expression with {func}`.create_parametrized_function`. After that we, update the parameters with the optimized parameter values we found in {ref}`usage/ampform:Perform fit`. The two components in the example below should be the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "added_components = model.sum_components(\n",
    "    components=[\n",
    "        R\"A_{J/\\psi(1S)_{+1} \\to f_{0}(500)_{0} \\gamma_{+1}; f_{0}(500)_{0}\"\n",
    "        R\" \\to \\pi^{0}_{0} \\pi^{0}_{0}}\",\n",
    "        R\"A_{J/\\psi(1S)_{+1} \\to f_{0}(980)_{0} \\gamma_{+1}; f_{0}(980)_{0}\"\n",
    "        R\" \\to \\pi^{0}_{0} \\pi^{0}_{0}}\",\n",
    "        R\"A_{J/\\psi(1S)_{+1} \\to f_{0}(1370)_{0} \\gamma_{+1}; f_{0}(1370)_{0}\"\n",
    "        R\" \\to \\pi^{0}_{0} \\pi^{0}_{0}}\",\n",
    "        R\"A_{J/\\psi(1S)_{+1} \\to f_{0}(1500)_{0} \\gamma_{+1}; f_{0}(1500)_{0}\"\n",
    "        R\" \\to \\pi^{0}_{0} \\pi^{0}_{0}}\",\n",
    "        R\"A_{J/\\psi(1S)_{+1} \\to f_{0}(1710)_{0} \\gamma_{+1}; f_{0}(1710)_{0}\"\n",
    "        R\" \\to \\pi^{0}_{0} \\pi^{0}_{0}}\",\n",
    "    ]\n",
    ")\n",
    "from_amplitudes = create_parametrized_function(\n",
    "    expression=added_components.doit(),\n",
    "    parameters=model.parameter_defaults,\n",
    "    backend=\"numpy\",\n",
    ")\n",
    "from_amplitudes.update_parameters(latest_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_components = model.sum_components(\n",
    "    components=[R\"I_{J/\\psi(1S)_{+1} \\to \\gamma_{+1} \\pi^{0}_{0} \\pi^{0}_{0}}\"]\n",
    ")\n",
    "from_intensity = create_parametrized_function(\n",
    "    expression=added_components.doit(),\n",
    "    parameters=model.parameter_defaults,\n",
    "    backend=\"numpy\",\n",
    ")\n",
    "from_intensity.update_parameters(latest_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "difference = np.average(from_amplitudes(phsp) - from_intensity(phsp))\n",
    "assert np.round(difference, decimals=15) == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a {class}`.ParametrizedBackendFunction` that can be plotted just like in {ref}`usage/ampform:Plot optimized model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 5))\n",
    "bins = 150\n",
    "ax.hist(\n",
    "    phsp[\"m_12\"],\n",
    "    weights=np.array(function(phsp)),\n",
    "    bins=bins,\n",
    "    alpha=0.2,\n",
    "    label=\"full intensity\",\n",
    ")\n",
    "ax.hist(\n",
    "    phsp[\"m_12\"],\n",
    "    weights=np.array(from_intensity(phsp)),\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    label=R\"$J/\\psi(1S)_{-1} \\to \\gamma_{-1} \\pi^0 \\pi^0$\",\n",
    ")\n",
    "ax.set_xlim(0.25, 2.5)\n",
    "ax.set_xlabel(R\"$m_{\\pi^0\\pi^0}$ [GeV]\")\n",
    "ax.set_yticks([])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or generically, so that we can stack all the sub-intensities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from tensorwaves.data import generate_data\n",
    "from tensorwaves.data.transform import SympyDataTransformer\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "intensity_components = [\n",
    "    create_parametrized_function(\n",
    "        expression=model.sum_components([c]).doit(),\n",
    "        parameters=model.parameter_defaults,\n",
    "        backend=\"numpy\",\n",
    "    )\n",
    "    for c in model.components\n",
    "    if c.startswith(\"I\")\n",
    "]\n",
    "initial_state_mass = reaction_info.initial_state[-1].mass\n",
    "final_state_masses = {i: p.mass for i, p in reaction_info.final_state.items()}\n",
    "\n",
    "helicity_transformer = SympyDataTransformer.from_sympy(\n",
    "    model.kinematic_variables, backend=\"numpy\"\n",
    ")\n",
    "masses = []\n",
    "for component in intensity_components:\n",
    "    sub_events = generate_data(\n",
    "        size=5_000,\n",
    "        initial_state_mass=initial_state_mass,\n",
    "        final_state_masses=final_state_masses,\n",
    "        data_transformer=helicity_transformer,\n",
    "        intensity=component,\n",
    "    )\n",
    "    sub_dataset = helicity_transformer(sub_events)\n",
    "    masses.append(sub_dataset[\"m_12\"])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 5))\n",
    "plt.hist(\n",
    "    masses,\n",
    "    bins=100,\n",
    "    stacked=True,\n",
    "    alpha=0.6,\n",
    ")\n",
    "ax.set_xlim(0.25, 2.5)\n",
    "ax.set_xlabel(R\"$m_{\\pi^0\\pi^0}$ [GeV]\")\n",
    "ax.set_yticks([])\n",
    "ax.legend(\n",
    "    labels=[f\"${c[3:-1]}$\" for c in model.components if c.startswith(\"I\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze optimization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that {ref}`in Step 3.2 <usage/ampform:3.2 Optimize intensity model>`, we initialized {class}`.Minuit2` with a {class}`.TFSummary` callback as well. Its output files provide a nice, interactive representation of the fit process and can be viewed with [TensorBoard](https://www.tensorflow.org/tensorboard/get_started) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tabbed} Terminal\n",
    "```bash\n",
    "tensorboard --logdir logs\n",
    "```\n",
    "````\n",
    "\n",
    "````{tabbed} Python\n",
    "```python\n",
    "import tensorboard as tb\n",
    "\n",
    "tb.notebook.list()  # View open TensorBoard instances\n",
    "tb.notebook.start(args_string=\"--logdir logs\")\n",
    "```\n",
    "See more info [here](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks#tensorboard_in_notebooks)\n",
    "````\n",
    "\n",
    "````{tabbed} Jupyter notebook\n",
    "```ipython\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "```\n",
    "See more info [here](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks#tensorboard_in_notebooks)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative would be to use the output of the {class}`.CSVSummary` callback. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sympy as sp\n",
    "\n",
    "converters = {p: lambda s: complex(s).real for p in initial_parameters}\n",
    "fit_traceback = pd.read_csv(\"fit_traceback_minuit.csv\", converters=converters)\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, figsize=(7, 8), gridspec_kw={\"height_ratios\": [1, 1.8]}\n",
    ")\n",
    "fit_traceback.plot(\"function_call\", \"estimator_value\", ax=ax1)\n",
    "fit_traceback.plot(\"function_call\", sorted(initial_parameters), ax=ax2)\n",
    "fig.suptitle(\"Minuit optimizer process\", fontsize=16)\n",
    "ax1.set_title(\"Negative log likelihood\")\n",
    "ax2.set_title(\"Parameter values\")\n",
    "ax1.set_xlabel(\"function call\")\n",
    "ax2.set_xlabel(\"function call\")\n",
    "fig.tight_layout()\n",
    "ax1.legend().remove()\n",
    "legend_texts = ax2.legend().get_texts()\n",
    "for text in legend_texts:\n",
    "    latex = f\"${sp.latex(sp.Symbol(text.get_text()))}$\"\n",
    "    latex = latex.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    if latex[2] == \"C\":\n",
    "        latex = fR\"\\left|{latex}\\right|\"\n",
    "    text.set_text(latex)\n",
    "for line in ax2.get_lines():\n",
    "    label = line.get_label()\n",
    "    color = line.get_color()\n",
    "    ax2.axhline(\n",
    "        y=complex(original_parameters[label]).real,\n",
    "        color=color,\n",
    "        alpha=0.5,\n",
    "        linestyle=\"dotted\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SciPy optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to the {class}`.Minuit2` optimizer, you can use the {class}`.ScipyMinimizer`. As opposed to Minuit, this optimizer does not compute errors. See {doc}`scipy:tutorial/optimize` and {mod}`scipy.optimize` for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorwaves.optimizer.scipy import ScipyMinimizer\n",
    "\n",
    "scipy = ScipyMinimizer(\n",
    "    callback=CSVSummary(\n",
    "        \"fit_traceback_scipy.csv\",\n",
    "        function_call_step_size=1,\n",
    "        iteration_step_size=1,\n",
    "    ),\n",
    "    use_analytic_gradient=False,\n",
    ")\n",
    "fit_result = scipy.optimize(estimator, initial_parameters)\n",
    "fit_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "converters = {p: lambda s: complex(s).real for p in initial_parameters}\n",
    "fit_traceback = pd.read_csv(\"fit_traceback_scipy.csv\", converters=converters)\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, figsize=(7, 8), gridspec_kw={\"height_ratios\": [1, 1.8]}\n",
    ")\n",
    "fit_traceback.plot(\"function_call\", \"estimator_value\", ax=ax1)\n",
    "fit_traceback.plot(\"function_call\", sorted(initial_parameters), ax=ax2)\n",
    "fig.suptitle(\"SciPy optimizer process\", fontsize=16)\n",
    "ax1.set_title(\"Negative log likelihood\")\n",
    "ax2.set_title(\"Parameter values\")\n",
    "ax1.set_xlabel(\"function call\")\n",
    "ax2.set_xlabel(\"function call\")\n",
    "fig.tight_layout()\n",
    "ax1.legend().remove()\n",
    "legend_texts = ax2.legend().get_texts()\n",
    "for text in legend_texts:\n",
    "    latex = f\"${sp.latex(sp.Symbol(text.get_text()))}$\"\n",
    "    latex = latex.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    if latex[2] == \"C\":\n",
    "        latex = fR\"\\left|{latex}\\right|\"\n",
    "    text.set_text(latex)\n",
    "for line in ax2.get_lines():\n",
    "    label = line.get_label()\n",
    "    color = line.get_color()\n",
    "    ax2.axhline(\n",
    "        y=complex(original_parameters[label]).real,\n",
    "        color=color,\n",
    "        alpha=0.5,\n",
    "        linestyle=\"dotted\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
