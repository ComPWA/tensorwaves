{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# WARNING: advised to install a specific version, e.g. tensorwaves==0.1.2\n",
    "%pip install -q tensorwaves[doc,jax,pwa,viz] IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import os\n",
    "\n",
    "from IPython.display import display  # noqa: F401\n",
    "\n",
    "STATIC_WEB_PAGE = {\"EXECUTE_NB\", \"READTHEDOCS\"}.intersection(os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{autolink-concat}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core ideas illustrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At core, {mod}`tensorwaves` is a package that can 'fit' arbitrary mathematical expressions to a data set using different computational back-ends. It can also use those expressions to describe a distribution over which to generate data samples.\n",
    "\n",
    "This page illustrate what's going on behind the scenes with some simple 1-dimensional and 2-dimensional expressions. The main steps are:\n",
    "\n",
    "1. Formulate a mathematical expression with {mod}`sympy`.\n",
    "2. Generate a distribution data sample for that expression.\n",
    "3. Express the expression as a function in some computational back-end.\n",
    "4. Tweak the {attr}`~.ParametrizedFunction.parameters` and fit the {class}`.ParametrizedFunction` to the generated distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip -q install black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as sp\n",
    "from sympy.plotting import plot3d\n",
    "\n",
    "from tensorwaves.estimator import UnbinnedNLL\n",
    "from tensorwaves.function.sympy import create_parametrized_function\n",
    "from tensorwaves.optimizer import Minuit2, ScipyMinimizer\n",
    "from tensorwaves.optimizer.callbacks import (\n",
    "    CallbackList,\n",
    "    CSVSummary,\n",
    "    TFSummary,\n",
    "    YAMLSummary,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Formulate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we'll formulate some expression with {mod}`sympy`. In this example, we take a sum of [Gaussians](https://en.wikipedia.org/wiki/Gaussian_function) plus some [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gaussian(x: sp.Symbol, mu: sp.Symbol, sigma: sp.Symbol) -> sp.Expr:\n",
    "    return sp.exp(-(((x - mu) / sigma) ** 2) / 2)\n",
    "\n",
    "\n",
    "def poisson(x: sp.Symbol, k: sp.Symbol) -> sp.Expr:\n",
    "    return x**k * sp.exp(-x) / sp.factorial(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x, mu, sigma = sp.symbols(\"x mu sigma\")\n",
    "k = sp.Symbol(\"k\", integer=True)\n",
    "lam = sp.Symbol(\"lambda\", positive=True)\n",
    "style = \"<style>#output-body{display:flex; flex-direction: row;}</style>\"\n",
    "display(\n",
    "    gaussian(x, mu, sigma),\n",
    "    poisson(lam, k),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, a, b, c, mu1, mu2, sigma1, sigma2 = sp.symbols(\"x (a:c) mu_(:2) sigma_(:2)\")\n",
    "expression_1d = (\n",
    "    a * gaussian(x, mu1, sigma1)\n",
    "    + b * gaussian(x, mu2, sigma2)\n",
    "    + c * poisson(x, k=2)\n",
    ")\n",
    "expression_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression above consists of a number of {class}`~sympy.core.symbol.Symbol`s that we want to identify as **parameters** (that we want to optimize with regard to a certain data sample) and **variables** (in which the data sample is expressed). Let's say $x$ is the variable and that the rest of the {class}`~sympy.core.symbol.Symbol`s are the parameters.\n",
    "\n",
    "Here, we'll pick some default values for the parameter and use them to plot the model with regard to the variable $x$ (see {meth}`~sympy.core.basic.Basic.subs`). The default values are used later on as well when we generate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "We use {attr}`~sympy.core.basic.Basic.args` here to extract the components of the sum that forms this expression. See {doc}`sympy:tutorial/manipulation`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_defaults = {\n",
    "    a: 0.15,\n",
    "    b: 0.05,\n",
    "    c: 0.3,\n",
    "    mu1: 1.0,\n",
    "    sigma1: 0.3,\n",
    "    mu2: 2.7,\n",
    "    sigma2: 0.5,\n",
    "}\n",
    "x_range = (x, 0, 5)\n",
    "substituted_expr_1d = expression_1d.subs(parameter_defaults)\n",
    "p1 = sp.plot(substituted_expr_1d, x_range, show=False, line_color=\"red\")\n",
    "p2 = sp.plot(*substituted_expr_1d.args, x_range, show=False, line_color=\"gray\")\n",
    "p2.append(p1[0])\n",
    "p2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all we did was using {mod}`sympy` to symbolically formulate a mathematical expression. We now need to {func}`~sympy.utilities.lambdify.lambdify` that expression to some computational backend, so that we can efficiently generate data and/or optimize the parameters in the function to 'fit' the model to some data sample. TensorWaves can do this with the {func}`.create_parametrized_function` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_1d = create_parametrized_function(\n",
    "    expression=expression_1d,\n",
    "    parameters=parameter_defaults,\n",
    "    backend=\"jax\",\n",
    "    use_cse=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "\n",
    "<!-- cspell:ignore subexpression -->\n",
    "Here, we used `use_cse=False` in {func}`.create_parametrized_function`. Setting this argument to `True` (the default) causes {mod}`sympy` to search for common sub-expressions, which speeds up lambdification in large expressions and makes the lambdified source code more efficient. See also {func}`~sympy.simplify.cse_main.cse`.\n",
    "\n",
    ":::\n",
    "\n",
    "The resulting {class}`.ParametrizedBackendFunction` internally carries some source code that {mod}`numpy` understands. With {func}`.get_source_code`, we can see that it indeed looks similar to the expression that we formulated in {ref}`usage/basics:Formulate model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from black import FileMode, format_str\n",
    "\n",
    "from tensorwaves.function import get_source_code\n",
    "\n",
    "src = get_source_code(function_1d)\n",
    "src = format_str(src, mode=FileMode())\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {class}`.ParametrizedBackendFunction` also carries the original default values for the {attr}`~.ParametrizedBackendFunction.parameters` that we defined earlier on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_1d.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {meth}`.ParametrizedFunction.__call__` takes a {class}`dict` of variable names (here, `\"x\"` only) to the value(s) that should be used in their place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_1d({\"x\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we move to the data generation â€• the input values are usually a list of values (expressed in the backend):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "x_values = np.linspace(0, 5, num=20)\n",
    "y_values = function_1d({\"x\": x_values})\n",
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_values, y_values)\n",
    "plt.gca().set_xlabel(\"$x$\")\n",
    "plt.gca().set_ylabel(\"$f(x)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now have a function $f$ of $x$ expressed in some computational backend. This function is to describe a distribution over $x$. In the real world, $x$ is an observable from a process you measure. But sometimes, it's useful to generate a 'toy' data sample for your model function as well, to try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hit & miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is to generate values of $x$ with a density that is proportional to the value of the function evaluated at that point. To do this, we use a **hit & miss** approach:\n",
    "\n",
    "1. Generate a random value for $x$ within the domain $(x_\\mathrm{min}, x_\\mathrm{max})$ on which you want to generate the data sample.\n",
    "2. Generate a random value $y$ between $0$ and the maximum value $y_\\mathrm{max}$ of the function over the domain of $x$.\n",
    "3. Check if $y$ lies below $f(x)$ (\"hit\") or above (\"miss\").\n",
    "4. If there is a \"hit\", accept this value of $x$ and add it to the data sample.\n",
    "\n",
    "We keep performing this until the sample of $x$ values contains the desired number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x_domain = np.linspace(0, 5, num=200)\n",
    "y_values = function_1d({\"x\": x_domain})\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_domain, y_values)\n",
    "plt.gca().set_xlabel(\"$x$\")\n",
    "plt.gca().set_ylabel(\"$f(x)$\")\n",
    "\n",
    "x_min = x_range[1]\n",
    "x_max = x_range[2]\n",
    "y_max = 0.21\n",
    "x_value = 1.5\n",
    "\n",
    "line_segment = [[0, 0], [0, y_max]]\n",
    "plt.plot(*line_segment, color=\"black\")\n",
    "plt.text(\n",
    "    -0.22,\n",
    "    y_max / 2 * 0.5,\n",
    "    \"uniform sample $(0, y_{max})$\",\n",
    "    rotation=\"vertical\",\n",
    ")\n",
    "plt.axhline(y=y_max, linestyle=\"dotted\", color=\"black\")\n",
    "plt.text(\n",
    "    x_min + 0.1,\n",
    "    y_max - 0.01,\n",
    "    \"$y_{max}$\",\n",
    ")\n",
    "\n",
    "line_segment = [[x_min, x_max], [0, 0]]\n",
    "plt.plot(*line_segment, color=\"black\")\n",
    "plt.text(\n",
    "    (x_max - x_min) / 2 - 0.22,\n",
    "    0.005,\n",
    "    R\"uniform sample $(x_\\mathrm{min}, x_\\mathrm{max})$\",\n",
    ")\n",
    "plt.scatter(x_value, function_1d({\"x\": x_value}))\n",
    "plt.axvline(x=x_value, linestyle=\"dotted\")\n",
    "\n",
    "\n",
    "def draw_y_hit(x_random, y_random):\n",
    "    y_value = function_1d({\"x\": x_random})\n",
    "    color = \"green\" if y_random < y_value else \"red\"\n",
    "    text = \"hit\" if y_random < y_value else \"miss\"\n",
    "    plt.scatter(0, y_random, color=color)\n",
    "    plt.arrow(\n",
    "        x=0,\n",
    "        y=y_random,\n",
    "        dx=x_random,\n",
    "        dy=0,\n",
    "        head_length=0.15,\n",
    "        length_includes_head=True,\n",
    "        color=color,\n",
    "        linestyle=\"dotted\",\n",
    "    )\n",
    "    plt.text(x_value + 0.05, y_random, text)\n",
    "\n",
    "\n",
    "draw_y_hit(x_random=x_value, y_random=0.05)\n",
    "draw_y_hit(x_random=x_value, y_random=0.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one problem though: _how to determine $y_\\mathrm{max}$?_ In this example, we can just read off the value of $y_\\mathrm{max}$, or even compute it analytically from the original {class}`sympy.Expr <sympy.core.expr.Expr>`. This is not the case generally though, so we need to apply a trick.\n",
    "\n",
    "Since we are generating **uniformly distributed** random values values of $x$ and computing their $f(x)$ values, we can keep track of which values of $f(x)$ is the highest. Starting with $y_\\mathrm{max} = 0$ we just set $y_\\mathrm{max} = f(x)$ once $f(x) > y_\\mathrm{max}$ and _completely restart_ the generate loop. Eventually, some value of $x$ will lie near the absolute maximum of $f$ and the data generation will happily continue until the requested number of events has been reached.\n",
    "\n",
    "```{warning}\n",
    "There are two caveats:\n",
    "1. The domain sample (here: the uniformly distributed values of $x$) has to be large in order for the data sample to accurately describe the original function.\n",
    "2. The the function displays narrow structures, like some sharp global maximum containing $y_\\mathrm{max}$, changes are smaller that the value of $x$ will lie within this peak. The domain sample will therefore have to be even larger. It will also take longer before $y_\\mathrm{max}$ is found.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to randomly generate values of $x$. In this simple, 1-dimensional example, we could just use a random generator like {class}`numpy.random.Generator` feed its output to the {meth}`.ParametrizedFunction.__call__`. Generally, though, we want to cover $n$-dimensional cases. The class {class}`.NumpyDomainGenerator` allows us to generate such a **uniform** distribution for each variable within a certain range. It requires a {class}`.RealNumberGenerator` (here we use {class}`.NumpyUniformRNG`) and it also requires us to define boundaries for each variable in the resulting {obj}`.DataSample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorwaves.data import NumpyDomainGenerator, NumpyUniformRNG\n",
    "\n",
    "rng = NumpyUniformRNG(seed=0)\n",
    "domain_generator = NumpyDomainGenerator(boundaries={\"x\": (0, 5)})\n",
    "domain = domain_generator.generate(1_000_000, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "\n",
    "Set a `seed` in the {class}`.RealNumberGenerator` if you want to generate **deterministic** data sample. If you leave it unspecified, you get an **indeterministic** data sample.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we feed the sample generated domain sample to the {class}`.ParametrizedBackendFunction` and use it its output values as weights to the histogram of the uniform domain sample, we see that the domain nicely produces a distribution as expected from the {ref}`model we defined <usage/basics:Formulate model>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    domain[\"x\"],\n",
    "    bins=200,\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    label=\"uniform\",\n",
    ")\n",
    "plt.hist(\n",
    "    domain[\"x\"],\n",
    "    weights=np.array(function_1d(domain)),\n",
    "    bins=200,\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    "    label=\"weighted with $f$\",\n",
    ")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "In PWA, the sample on which we perform hit-and-miss is not uniform, because the available space is limited by the masses of the initial and final state (phase space). See {class}`.TFPhaseSpaceGenerator` and {ref}`amplitude-analysis:Step 2: Generate data`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a {ref}`usage/basics:Domain distribution` in hand, we can work out an implementation for the {ref}`usage/basics:Hit & miss` approach. The {class}`.IntensityDistributionGenerator` class helps us to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorwaves.data import IntensityDistributionGenerator\n",
    "\n",
    "data_generator = IntensityDistributionGenerator(domain_generator, function_1d)\n",
    "data = data_generator.generate(1_000_000, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, it results in the correct distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(data[\"x\"], bins=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest, the procedure is really just the same as that sketched in {ref}`compwa-step-3`.\n",
    "\n",
    "We tweak the parameters a bit, then use {meth}`.ParametrizedBackendFunction.update_parameters` to change the function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_parameters = {\n",
    "    \"a\": 0.2,\n",
    "    \"b\": 0.1,\n",
    "    \"c\": 0.2,\n",
    "    \"mu_0\": 0.9,\n",
    "    \"sigma_0\": 0.4,\n",
    "    \"sigma_1\": 0.4,\n",
    "}\n",
    "function_1d.update_parameters(initial_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...compare what this looks like compared to the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(data[\"x\"], bins=200, density=True)\n",
    "plt.hist(\n",
    "    domain[\"x\"],\n",
    "    weights=np.array(function_1d(domain)),\n",
    "    bins=200,\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    density=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...define an {class}`.Estimator` and choose {obj}`jax <jax.jit>` as backend..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = UnbinnedNLL(function_1d, data, domain, backend=\"jax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...optimize with {class}`.Minuit2` (the `callback` argument is optional---see {ref}`usage/basics:Callbacks`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minuit2 = Minuit2(\n",
    "    callback=CallbackList(\n",
    "        [\n",
    "            CSVSummary(\"traceback-1D.csv\"),\n",
    "            YAMLSummary(\"fit-result-1D.yaml\"),\n",
    "            TFSummary(),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "fit_result = minuit2.optimize(estimator, initial_parameters)\n",
    "fit_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert fit_result.minimum_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "\n",
    "For complicated expressions, the fit can be made faster with {func}`.create_cached_function`. See {doc}`/usage/caching`.\n",
    "\n",
    ":::\n",
    "\n",
    "And again, we have a look at the resulting fit, as well as what happened during the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_parameters = fit_result.parameter_values\n",
    "function_1d.update_parameters(optimized_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(data[\"x\"], bins=200, density=True)\n",
    "plt.hist(\n",
    "    domain[\"x\"],\n",
    "    weights=np.array(function_1d(domain)),\n",
    "    bins=200,\n",
    "    histtype=\"step\",\n",
    "    color=\"red\",\n",
    "    density=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "The {class}`.Minuit2` optimizer above was constructed with {mod}`.callbacks`. Callbacks allow us to insert behavior into the fit procedure of the optimizer. In this example, we use {class}`.CallbackList` to stack some {class}`.Callback` classes: {class}`.CSVSummary`, {class}`.YAMLSummary`, and {class}`.TFSummary`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " {class}`.YAMLSummary` writes the latest fit result to disk. It's a {class}`.Loadable` callable and can be used to **pick up a fit later on**, for instance if it was aborted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latest_parameters = YAMLSummary.load_latest_parameters(\"fit-result-1D.yaml\")\n",
    "latest_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = Minuit2()\n",
    "optimizer.optimize(estimator, latest_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{class}`.CSVSummary` records the parameter values in each iteration and can be used to **analyze the fit process**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fit_traceback = pd.read_csv(\"traceback-1D.csv\")\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, figsize=(7, 9), sharex=True, gridspec_kw={\"height_ratios\": [1, 2]}\n",
    ")\n",
    "fit_traceback.plot(\"function_call\", \"estimator_value\", ax=ax1)\n",
    "fit_traceback.plot(\"function_call\", sorted(initial_parameters), ax=ax2)\n",
    "fig.tight_layout()\n",
    "ax2.set_xlabel(\"function call\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{class}`.TFSummary` provides a nice, **interactive representation of the fit process** and can be viewed with [TensorBoard](https://www.tensorflow.org/tensorboard/get_started) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tabbed} Terminal\n",
    "```bash\n",
    "tensorboard --logdir logs\n",
    "```\n",
    "````\n",
    "\n",
    "````{tabbed} Python\n",
    "```python\n",
    "import tensorboard as tb\n",
    "\n",
    "tb.notebook.list()  # View open TensorBoard instances\n",
    "tb.notebook.start(args_string=\"--logdir logs\")\n",
    "```\n",
    "See more info [here](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks#tensorboard_in_notebooks)\n",
    "````\n",
    "\n",
    "````{tabbed} Jupyter notebook\n",
    "```ipython\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "```\n",
    "See more info [here](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks#tensorboard_in_notebooks)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea illustrated above works for any number of dimensions. Let's create multiply the expression we had with some $\\cos$ as a function of $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, omega = sp.symbols(\"y omega\")\n",
    "expression_2d = expression_1d * sp.cos(y * omega) ** 2\n",
    "expression_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_defaults[omega] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = (y, -sp.pi, +sp.pi)\n",
    "substituted_expr_2d = expression_2d.subs(parameter_defaults)\n",
    "plot3d(substituted_expr_2d, x_range, y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_2d = create_parametrized_function(\n",
    "    expression=expression_2d,\n",
    "    parameters=parameter_defaults,\n",
    "    backend=\"jax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boundaries = {\"x\": (0, 5), \"y\": (-np.pi, +np.pi)}\n",
    "domain_generator_2d = NumpyDomainGenerator(boundaries)\n",
    "data_generator_2d = IntensityDistributionGenerator(\n",
    "    domain_generator_2d, function_2d\n",
    ")\n",
    "domain_2d = domain_generator_2d.generate(300_000, rng)\n",
    "data_2d = data_generator_2d.generate(30_000, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "intensities = np.array(function_2d(domain_2d))\n",
    "kwargs = {\n",
    "    \"weights\": intensities,\n",
    "    \"bins\": 100,\n",
    "    \"density\": True,\n",
    "}\n",
    "axes[0].hist(domain_2d[\"x\"], **kwargs)\n",
    "axes[1].hist(domain_2d[\"y\"], **kwargs)\n",
    "axes[0].set_xlabel(\"$x$\")\n",
    "axes[1].set_xlabel(\"$y$\")\n",
    "axes[0].set_ylabel(\"$f(x, y)$\")\n",
    "axes[0].set_yticks([])\n",
    "axes[1].set_yticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform fit with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_parameters = {\n",
    "    \"a\": 0.1,\n",
    "    \"b\": 0.1,\n",
    "    \"c\": 0.2,\n",
    "    \"mu_0\": 0.9,\n",
    "    \"omega\": 0.35,\n",
    "    \"sigma_0\": 0.4,\n",
    "    \"sigma_1\": 0.4,\n",
    "}\n",
    "function_2d.update_parameters(initial_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4), sharey=True, tight_layout=True)\n",
    "axes[0].hist2d(**data_2d, bins=50)\n",
    "axes[1].hist2d(**domain_2d, weights=function_2d(domain_2d), bins=50)\n",
    "axes[0].set_xlabel(\"$x$\")\n",
    "axes[0].set_ylim([-3, +3])\n",
    "axes[1].set_xlabel(\"$x$\")\n",
    "axes[0].set_ylabel(\"$y$\")\n",
    "axes[0].set_title(\"Data sample\")\n",
    "axes[1].set_title(\"Function with optimized parameters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minuit2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly, one would construct a {class}`.Minuit2` instance and call its {meth}`~.Minuit2.optimize` method. For more advanced options, one could specify a small `minuit_modifier` protocol into the {class}`.Minuit2` constructor. In this example, we set the {attr}`~iminuit.Minuit.tol` attribute. For other options, see {class}`iminuit.Minuit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_minuit(minuit) -> None:\n",
    "    minuit.tol = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest, the fit procedure goes just as in {ref}`usage:Optimize parameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = UnbinnedNLL(function_2d, data_2d, domain_2d, backend=\"jax\")\n",
    "minuit2 = Minuit2(\n",
    "    callback=CSVSummary(\"traceback.csv\"),\n",
    "    minuit_modifier=tweak_minuit,\n",
    ")\n",
    "fit_result = minuit2.optimize(estimator, initial_parameters)\n",
    "fit_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that further information about the internal {class}`iminuit.Minuit` optimizer is available through {attr}`.FitResult.specifics`, e.g. computing the {meth}`~iminuit.Minuit.hesse` afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result.specifics.hesse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scipy = ScipyMinimizer(\n",
    "    method=\"Nelder-Mead\",\n",
    "    callback=CSVSummary(\"traceback-scipy.csv\"),\n",
    ")\n",
    "fit_result = scipy.optimize(estimator, initial_parameters)\n",
    "fit_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "\n",
    "Scipy does not provide error values for the optimized parameters.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze fit process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we update the parameters in the {class}`.ParametrizedFunction` with the optimized parameter values found by the {class}`.Optimizer`, we can compare the data distribution with the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_parameters = fit_result.parameter_values\n",
    "function_2d.update_parameters(optimized_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4), sharey=True, tight_layout=True)\n",
    "fig.suptitle(\"Final fit result\")\n",
    "axes[0].hist2d(**data_2d, bins=50)\n",
    "axes[1].hist2d(**domain_2d, weights=function_2d(domain_2d), bins=50)\n",
    "axes[0].set_xlabel(\"$x$\")\n",
    "axes[0].set_ylim([-3, +3])\n",
    "axes[1].set_xlabel(\"$x$\")\n",
    "axes[0].set_ylabel(\"$y$\")\n",
    "axes[0].set_title(\"Data sample\")\n",
    "axes[1].set_title(\"Function with optimized parameters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the {ref}`callbacks <usage/basics:Callbacks>` allow us to inspect how the parameter values evolved during the fit with the {class}`.ScipyMinimizer` and {class}`.Minuit2` optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "minuit_traceback = pd.read_csv(\"traceback.csv\")\n",
    "scipy_traceback = pd.read_csv(\"traceback-scipy.csv\")\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(\n",
    "    ncols=2,\n",
    "    nrows=2,\n",
    "    figsize=(10, 9),\n",
    "    gridspec_kw={\"height_ratios\": [1, 3]},\n",
    ")\n",
    "fig.suptitle(\"Evolution of the parameter values during the fit\")\n",
    "ax1.set_title(\"Minuit2\")\n",
    "ax2.set_title(\"Scipy (Nelder-Mead)\")\n",
    "ax1.get_shared_x_axes().join(ax1, ax3)\n",
    "ax2.get_shared_x_axes().join(ax2, ax4)\n",
    "ax1.get_shared_y_axes().join(ax1, ax2)\n",
    "ax3.get_shared_y_axes().join(ax3, ax4)\n",
    "ax2.set_ylim(\n",
    "    1.02 * scipy_traceback[\"estimator_value\"].min(),\n",
    "    0.98 * scipy_traceback[\"estimator_value\"].max(),\n",
    ")\n",
    "minuit_traceback.plot(\"function_call\", \"estimator_value\", ax=ax1, legend=False)\n",
    "scipy_traceback.plot(\"function_call\", \"estimator_value\", ax=ax2)\n",
    "minuit_traceback.plot(\n",
    "    \"function_call\", initial_parameters, ax=ax3, legend=False\n",
    ")\n",
    "scipy_traceback.plot(\n",
    "    \"function_call\", initial_parameters, ax=ax4, legend=True\n",
    ").legend(loc=\"upper right\")\n",
    "fig.tight_layout()\n",
    "ax2.set_xlabel(\"function call\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
