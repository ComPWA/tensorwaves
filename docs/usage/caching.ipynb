{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# WARNING: advised to install a specific version, e.g. tensorwaves==0.1.2\n",
    "%pip install -q tensorwaves[doc,jax,pwa,viz] IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import os\n",
    "\n",
    "from IPython.display import display  # noqa: F401\n",
    "\n",
    "STATIC_WEB_PAGE = {\"EXECUTE_NB\", \"READTHEDOCS\"}.intersection(os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{autolink-concat}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant sub-expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in {ref}`amplitude-analysis:3.1 Prepare parametrized function`, once we know which parameters in a {class}`.ParametrizedFunction` we want to optimize, we can apply several optimizations before running {meth}`~.Optimizer.optimize`. The most important of these is to identify sub-expressions that are unaffected by a change to one of the {attr}`~.ParametrizedFunction.parameters` (constant sub-expressions). It's smart to compute these sub-expressions beforehand (caching), so that only the top-expression has to be recomputed for each iteration of the {class}`.Optimizer`.\n",
    "\n",
    "If we are creating the {class}`.ParametrizedFunction` from a {class}`sympy.Expr <sympy.core.expr.Expr>`, the strategy is as follows:\n",
    "1. Create a top-expression where the constant sub-expressions are collapsed into constant nodes (represented by {class}`~sympy.core.symbol.Symbol`s) and a mapping of those {class}`~sympy.core.symbol.Symbol`s to the substituted sub-expressions. This can be done with {func}`.extract_constant_sub_expressions`.\n",
    "2. Create a new {class}`.ParametrizedFunction` for this top-expression and a {class}`.SympyDataTransformer` for the sub-expressions.\n",
    "3. Transform the original {obj}`.DataSample`s with that {class}`.SympyDataTransformer` (this is where the caching takes place).\n",
    "\n",
    "This procedure is facilitated with the function {func}`.create_cached_function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine free parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look how this works for a simple expression. Caching makes more sense in complicated expressions like the ones in {doc}`/amplitude-analysis`, but this simple expression illustrates the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "a, b, c, d, x, y = sp.symbols(\"a b c d x y\")\n",
    "expression = a * x + b * (c * x**2 + d * y**2)\n",
    "expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we have a data distribution over $x$ and that we _only_ want to optimize the **free parameters** $a$ and $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_symbols = {a, d}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we would just use {func}`.create_parametrized_function` over the entire expression without thinking about which {class}`~sympy.core.symbol.Symbol`s other than **variables** $x$ and $y$ are to be optimizable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorwaves.function.sympy import create_parametrized_function\n",
    "\n",
    "parameter_defaults = {a: -2.5, b: 1, c: 0.0, d: 3.7}\n",
    "original_func = create_parametrized_function(\n",
    "    expression,\n",
    "    parameter_defaults,\n",
    "    backend=\"numpy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that resulting {class}`.ParametrizedFunction` will have to compute the entire expression tree on each iteration, even though we only want to optimize the blue parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "\n",
    "class SymbolIdentifiable(sp.Symbol):\n",
    "    # SymbolIdentifiable because of alphabetical sorting in dotprint\n",
    "    @classmethod\n",
    "    def from_symbol(cls, symbol):\n",
    "        return SymbolIdentifiable(symbol.name, **symbol.assumptions0)\n",
    "\n",
    "\n",
    "dot_style = (\n",
    "    (sp.Basic, {\"color\": \"blue\", \"shape\": \"ellipse\"}),\n",
    "    (sp.Expr, {\"color\": \"black\"}),\n",
    "    (sp.Atom, {\"color\": \"gray\"}),\n",
    "    (SymbolIdentifiable, {\"color\": \"blue\"}),\n",
    ")\n",
    "\n",
    "\n",
    "def visualize_free_symbols(expression, free_symbols):\n",
    "    def substitute_identifiable_symbols(expression, symbols):\n",
    "        substitutions = {s: SymbolIdentifiable.from_symbol(s) for s in symbols}\n",
    "        return expression.xreplace(substitutions)\n",
    "\n",
    "    dot = sp.dotprint(\n",
    "        substitute_identifiable_symbols(expression, symbols=free_symbols),\n",
    "        styles=dot_style,\n",
    "        dpi=60,\n",
    "        bgcolor=\"none\",\n",
    "    )\n",
    "    graph = graphviz.Source(dot)\n",
    "    display(graph)\n",
    "\n",
    "\n",
    "visualize_free_symbols(expression, free_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract constant sub-expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function {func}`.extract_constant_sub_expressions` helps us to extract sub-expressions that remain constant with regard to some of its {class}`~sympy.core.symbol.Symbol`s. It returns a new top-expression where the sub-expressions are substituted by symbols $f_0, f_1, \\dots$, as well as a mapping with sub-expression definitions for these symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorwaves.function.sympy import extract_constant_sub_expressions\n",
    "\n",
    "top_expression, sub_expressions = extract_constant_sub_expressions(\n",
    "    expression, free_symbols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Math\n",
    "\n",
    "display(top_expression)\n",
    "for symbol, expr in sub_expressions.items():\n",
    "    latex = sp.multiline_latex(symbol, expr, environment=\"eqnarray\")\n",
    "    display(Math(latex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice how we have split up the original expression tree into a top tree with parameters that are to be optimized and sub-trees that remain constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_free_symbols(top_expression, free_symbols)\n",
    "for symbol, expr in sub_expressions.items():\n",
    "    dot = sp.dotprint(expr, dpi=60, styles=dot_style, bgcolor=\"none\")\n",
    "    display(graphviz.Source(dot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional optimization, we could further substitute the non-optimized parameters with the values to which they are fixed. This can be done with {func}`.prepare_caching`. Notice how one of the sub-expression trees disappears altogether, because we decided to choose $c=0$ in the `parameter_defaults` and how the top tree has been simplified since $b=1$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# see text in previous cell\n",
    "assert parameter_defaults[c] == 0\n",
    "assert parameter_defaults[b] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorwaves.function.sympy import prepare_caching\n",
    "\n",
    "cache_expression, transformer_expressions = prepare_caching(\n",
    "    expression, parameter_defaults, free_symbols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_free_symbols(cache_expression.evalf(2), free_symbols)\n",
    "for symbol, expr in transformer_expressions.items():\n",
    "    if expr is symbol:\n",
    "        continue\n",
    "    dot = sp.dotprint(expr, dpi=60, styles=dot_style, bgcolor=\"none\")\n",
    "    display(graphviz.Source(dot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above is mainly useful when {ref}`optimizing <usage/basics:Optimize the model>` a {class}`.ParametrizedFunction` with regard to some {class}`.Estimator`. For this reason, the {mod}`.estimator` module brings this all together with the function {func}`.create_cached_function`. This function prepares the expression trees just like we see above and creates a 'cached' {class}`.ParametrizedFunction` from the top-expression, as well as a {class}`.DataTransformer` to create a 'cached' {obj}`.DataSample` as input for that cached function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorwaves.estimator import create_cached_function\n",
    "\n",
    "cached_func, cache_transformer = create_cached_function(\n",
    "    expression,\n",
    "    parameter_defaults,\n",
    "    free_parameters=free_symbols,\n",
    "    backend=\"numpy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only the free parameters appear as {attr}`~.ParametrizedFunction.parameters` in the 'cached' function, how the {class}`.DataTransformer` defines the remaining symbols, and how variables $x, y$ are the only required arguments to the functions in the {class}`.DataTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cached_func.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_transformer.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "free_parameter_names = set(map(str, free_symbols))\n",
    "cache_variable_names = set(cached_func.argument_order) - free_parameter_names\n",
    "assert set(cached_func.parameters) == free_parameter_names\n",
    "assert set(cache_transformer.functions) == cache_variable_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use this 'cached' {class}`.ParametrizedFunction` and {class}`.DataTransformer`? And is the output of that function the same as the normal functions created with {func}`.create_parametrized_function`? Let's generate generate a small {obj}`.DataSample` for the domain $x, y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorwaves.data import NumpyDomainGenerator, NumpyUniformRNG\n",
    "\n",
    "boundaries = {\n",
    "    \"x\": (-1, +1),\n",
    "    \"y\": (-1, +1),\n",
    "}\n",
    "domain_generator = NumpyDomainGenerator(boundaries)\n",
    "rng = NumpyUniformRNG()\n",
    "domain = domain_generator.generate(10_000, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain {obj}`.DataSample` can be given directly to the original function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intensities = original_func(domain)\n",
    "intensities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'cached' function, we first need to transform the domain. **This is where the caching takes place!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cached_domain = cache_transformer(domain)\n",
    "intensities_from_cache = cached_func(cached_domain)\n",
    "intensities_from_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.testing.assert_allclose(intensities, intensities_from_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are indeed the same and the cached function is faster as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{autolink-skip}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit -n100 original_func(domain)\n",
    "%timeit -n100 cached_func(cached_domain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
